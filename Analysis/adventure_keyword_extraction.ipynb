{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d4732f8",
   "metadata": {},
   "source": [
    "# 빈도분석 + 키워드 추출\n",
    "## 불용어 리스트인 stopwords.txt 다운로드 필수\n",
    "## 키워드 추출 결과가 저장된 변수: result (리스트 형태)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "116b1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 import\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "from pykospacing import Spacing #pip install git+https://github.com/haven-jeon/PyKoSpacing.git \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "184a9ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도분석 + 키워드 추출 함수\n",
    "\n",
    "def load_area():\n",
    "    df = pd.read_csv('경기도 양평군.csv', encoding='utf-8')\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.reset_index(drop=True)\n",
    "    df_body = df.iloc[:, 1:2]\n",
    "    body = df_body['블로그내용']\n",
    "    return body\n",
    "\n",
    "def load_stopwords():\n",
    "    with open('stopwords.txt', 'r') as f:\n",
    "        list_file = f.readlines()\n",
    "    return list_file[0].split(\",\")\n",
    "\n",
    "# 정규 표현식을 통해 한글 단어만 남기기 (이모티콘, 초성, 영어 제거)\n",
    "def extract_word(text):\n",
    "    hangul = re.compile('[^가-힣]') \n",
    "    result = hangul.sub(' ', text)\n",
    "    return result\n",
    "\n",
    "def make_wordlist(body,stopwords):\n",
    "    #정규표현식 적용\n",
    "    print(\"데이터 정제 중....\")\n",
    "    words = body.apply(lambda x:extract_word(x))\n",
    "    spacing = Spacing()\n",
    "    words = words.apply(lambda x:spacing(x))\n",
    "    \n",
    "    #형태소 추출\n",
    "    print(\"형태소 추출 중....\")\n",
    "    okt = Okt()\n",
    "    words = \" \".join(words.tolist())\n",
    "    words = okt.morphs(words,stem=True)\n",
    "    \n",
    "    #한글자 제거\n",
    "    print(\"한글자 제거 중....\")\n",
    "    words = [x for x in words if len(x)>1]\n",
    "    \n",
    "    #불용어 제거\n",
    "    print(\"불용어 제거 중....\")\n",
    "    words = [x for x in words if x not in stopwords]\n",
    "    \n",
    "    #최소횟수 미만 제거\n",
    "    print(\"키워드 빈도 수 기준 상위 20개 리스트 생성 중....\")\n",
    "    time.sleep(1)\n",
    "    extraction = Counter(words).most_common()[0:20]\n",
    "    result = [x[0] for x in extraction]\n",
    "    return result\n",
    "\n",
    "\n",
    "def run():\n",
    "    body = load_area()\n",
    "    print(make_wordlist(body, load_stopwords()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1d43174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 정제 중....\n",
      "형태소 추출 중....\n",
      "한글자 제거 중....\n",
      "불용어 제거 중....\n",
      "키워드 빈도 수 기준 상위 20개 리스트 생성 중....\n",
      "['양평', '좋다', '여행', '먹다', '경기도', '지도', '양평군', '카페', '보기', '많다', '맛있다', '펜션', '자다', '거리', '영상', '사진', '허브', '복사', '도움말', '두물머리']\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af930d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
